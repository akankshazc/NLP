{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139d5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80308ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd1fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\": True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This is an email address: wgemmalemma@domain.com\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c28f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: [(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", matches) # Lexeme, start token, end token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c79367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[matches[0][0]].text)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdf6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SPACE\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[matches[0][1]].text)  # Print the matched text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b657866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_TITLE\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[matches[0][2]].text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532f8e5",
   "metadata": {},
   "source": [
    "# Grabbing all Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6e3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wiki_mlk.txt\", \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35c2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df638180",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUN\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344aba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 101\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e919e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451313080118390996, 0, 1) Martin\n",
      "(451313080118390996, 1, 2) Luther\n",
      "(451313080118390996, 2, 3) King\n",
      "(451313080118390996, 3, 4) Jr.\n",
      "(451313080118390996, 6, 7) Michael\n",
      "(451313080118390996, 7, 8) King\n",
      "(451313080118390996, 8, 9) Jr.\n",
      "(451313080118390996, 10, 11) January\n",
      "(451313080118390996, 15, 16) April\n",
      "(451313080118390996, 49, 50) King\n"
     ]
    }
   ],
   "source": [
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62353900",
   "metadata": {},
   "source": [
    "# Improving it with multi word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3322197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33428f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 174\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5ff09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 0, 1) Martin\n",
      "(3232560085755078826, 0, 2) Martin Luther\n",
      "(3232560085755078826, 1, 2) Luther\n",
      "(3232560085755078826, 0, 3) Martin Luther King\n",
      "(3232560085755078826, 1, 3) Luther King\n",
      "(3232560085755078826, 2, 3) King\n",
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 1, 4) Luther King Jr.\n",
      "(3232560085755078826, 2, 4) King Jr.\n",
      "(3232560085755078826, 3, 4) Jr.\n"
     ]
    }
   ],
   "source": [
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d7d73",
   "metadata": {},
   "source": [
    "Too many repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d741bb6",
   "metadata": {},
   "source": [
    "# Greedy Keyword Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b0507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1466ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c5e4717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 83, 88) Martin Luther King Sr.\n",
      "(3232560085755078826, 469, 474) Martin Luther King Jr. Day\n",
      "(3232560085755078826, 536, 541) Martin Luther King Jr. Memorial\n",
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 128, 132) Southern Christian Leadership Conference\n",
      "(3232560085755078826, 247, 251) Director J. Edgar Hoover\n",
      "(3232560085755078826, 6, 9) Michael King Jr.\n",
      "(3232560085755078826, 325, 328) Nobel Peace Prize\n",
      "(3232560085755078826, 422, 425) James Earl Ray\n",
      "(3232560085755078826, 463, 466) Congressional Gold Medal\n"
     ]
    }
   ],
   "source": [
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304b519",
   "metadata": {},
   "source": [
    "# Sorting it to appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecfed460",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d11070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef81a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 6, 9) Michael King Jr.\n",
      "(3232560085755078826, 10, 11) January\n",
      "(3232560085755078826, 15, 16) April\n",
      "(3232560085755078826, 49, 50) King\n",
      "(3232560085755078826, 69, 71) Mahatma Gandhi\n",
      "(3232560085755078826, 83, 88) Martin Luther King Sr.\n",
      "(3232560085755078826, 89, 90) King\n",
      "(3232560085755078826, 113, 114) King\n",
      "(3232560085755078826, 117, 118) Montgomery\n"
     ]
    }
   ],
   "source": [
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3838b3",
   "metadata": {},
   "source": [
    "# Adding in sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31b768b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf5e0f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f987e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 49, 51) King advanced\n",
      "(3232560085755078826, 89, 91) King participated\n",
      "(3232560085755078826, 113, 115) King led\n",
      "(3232560085755078826, 167, 169) King helped\n",
      "(3232560085755078826, 247, 252) Director J. Edgar Hoover considered\n",
      "(3232560085755078826, 322, 324) King won\n",
      "(3232560085755078826, 485, 488) United States beginning\n"
     ]
    }
   ],
   "source": [
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfa5ee",
   "metadata": {},
   "source": [
    "# Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cfdadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d604df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/alice.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f588cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed34bd3",
   "metadata": {},
   "source": [
    "Replacing non standard quotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "073263e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0].replace(\"`\", \"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28a9ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\": \"'\"}, \n",
    "{\"IS_ALPHA\": True, \"OP\": \"+\"}, \n",
    "{\"IS_PUNCT\": True, \"OP\": \"*\"}, \n",
    "{\"ORTH\": \"'\"}]\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40a403e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ffc0228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 47, 58) 'and what is the use of a book,'\n",
      "(3232560085755078826, 60, 67) 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a0883",
   "metadata": {},
   "source": [
    "## Find Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "003ccfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace(\"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"ORTH\": \"'\"},\n",
    "{\"IS_ALPHA\": True, \"OP\": \"+\"},\n",
    "{\"IS_PUNCT\": True, \"OP\": \"*\"},\n",
    "{\"ORTH\": \"'\"},\n",
    "{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},\n",
    "{\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
    "{\"ORTH\": \"'\"},\n",
    "{\"IS_ALPHA\": True, \"OP\": \"+\"},\n",
    "{\"IS_PUNCT\": True, \"OP\": \"*\"},\n",
    "{\"ORTH\": \"'\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04ed5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"PROPER_NOUNS\", [pattern1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7163c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8f4dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87e6a",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e28c463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# over the whole chapter\n",
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print(len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0118c",
   "metadata": {},
   "source": [
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d58db1",
   "metadata": {},
   "source": [
    "## Adding more patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71eec75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eec624ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'ORTH': \"'\"}, \n",
    "{'IS_ALPHA': True, \"OP\": \"+\"}, \n",
    "{'IS_PUNCT': True, \"OP\": \"*\"}, \n",
    "{'ORTH': \"'\"}, \n",
    "{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, \n",
    "{\"POS\": \"PROPN\", \"OP\": \"+\"}, \n",
    "{'ORTH': \"'\"}, \n",
    "{'IS_ALPHA': True, \"OP\": \"+\"}, \n",
    "{'IS_PUNCT': True, \"OP\": \"*\"}, \n",
    "{'ORTH': \"'\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bb1a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = [{'ORTH': \"'\"}, \n",
    "{'IS_ALPHA': True, \"OP\": \"+\"}, \n",
    "{'IS_PUNCT': True, \"OP\": \"*\"}, \n",
    "{'ORTH': \"'\"}, \n",
    "{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, \n",
    "{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b95c5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
    "{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, \n",
    "{'ORTH': \"'\"}, \n",
    "{'IS_ALPHA': True, \"OP\": \"+\"}, \n",
    "{'IS_PUNCT': True, \"OP\": \"*\"}, \n",
    "{'ORTH': \"'\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d02651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf632136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 0, 6) 'Well!' thought Alice\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 57, 68) 'which certainly was not here before,' said Alice\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print(len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1a70e",
   "metadata": {},
   "source": [
    "This is where being a domain expert and knowing the text comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276641db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
